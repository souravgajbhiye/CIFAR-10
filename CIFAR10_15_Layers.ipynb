{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qFM2YvbNn7n8",
    "outputId": "b821b6c6-1ca1-47c8-dab0-06eda0a77e7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/125\n",
      "781/781 [==============================] - 483s 619ms/step - loss: 1.8719 - accuracy: 0.4328 - val_loss: 1.3241 - val_accuracy: 0.5955\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 480s 615ms/step - loss: 1.2343 - accuracy: 0.5987 - val_loss: 1.1055 - val_accuracy: 0.6600\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 479s 613ms/step - loss: 1.0586 - accuracy: 0.6589 - val_loss: 0.9786 - val_accuracy: 0.7089\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 480s 615ms/step - loss: 0.9634 - accuracy: 0.6928 - val_loss: 0.8713 - val_accuracy: 0.7418\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 479s 614ms/step - loss: 0.9016 - accuracy: 0.7182 - val_loss: 0.8094 - val_accuracy: 0.7620\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 480s 615ms/step - loss: 0.8497 - accuracy: 0.7371 - val_loss: 0.7442 - val_accuracy: 0.7808\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 481s 615ms/step - loss: 0.8155 - accuracy: 0.7518 - val_loss: 0.7991 - val_accuracy: 0.7648\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 482s 617ms/step - loss: 0.7936 - accuracy: 0.7616 - val_loss: 0.7790 - val_accuracy: 0.7830\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 479s 613ms/step - loss: 0.7704 - accuracy: 0.7713 - val_loss: 0.7919 - val_accuracy: 0.7720\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 482s 617ms/step - loss: 0.7551 - accuracy: 0.7780 - val_loss: 0.8094 - val_accuracy: 0.7760\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 479s 614ms/step - loss: 0.7376 - accuracy: 0.7836 - val_loss: 0.7826 - val_accuracy: 0.7848\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 476s 609ms/step - loss: 0.7232 - accuracy: 0.7930 - val_loss: 0.7230 - val_accuracy: 0.8037\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 477s 610ms/step - loss: 0.7066 - accuracy: 0.7976 - val_loss: 0.7180 - val_accuracy: 0.8033\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 482s 617ms/step - loss: 0.7089 - accuracy: 0.7984 - val_loss: 0.6372 - val_accuracy: 0.8283\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 483s 619ms/step - loss: 0.6955 - accuracy: 0.8057 - val_loss: 0.6132 - val_accuracy: 0.8365\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 485s 621ms/step - loss: 0.6896 - accuracy: 0.8100 - val_loss: 0.6627 - val_accuracy: 0.8247\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 484s 620ms/step - loss: 0.6835 - accuracy: 0.8089 - val_loss: 0.7311 - val_accuracy: 0.8023\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 486s 623ms/step - loss: 0.6750 - accuracy: 0.8128 - val_loss: 0.6285 - val_accuracy: 0.8392\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 481s 615ms/step - loss: 0.6646 - accuracy: 0.8165 - val_loss: 0.6570 - val_accuracy: 0.8316\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 484s 620ms/step - loss: 0.6628 - accuracy: 0.8195 - val_loss: 0.6297 - val_accuracy: 0.8364\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 487s 623ms/step - loss: 0.6590 - accuracy: 0.8194 - val_loss: 0.6779 - val_accuracy: 0.8237\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 484s 619ms/step - loss: 0.6531 - accuracy: 0.8229 - val_loss: 0.5935 - val_accuracy: 0.8506\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 483s 619ms/step - loss: 0.6484 - accuracy: 0.8254 - val_loss: 0.7093 - val_accuracy: 0.8137\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 480s 615ms/step - loss: 0.6473 - accuracy: 0.8270 - val_loss: 0.6589 - val_accuracy: 0.8339\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 486s 622ms/step - loss: 0.6470 - accuracy: 0.8248 - val_loss: 0.6182 - val_accuracy: 0.8428\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 485s 621ms/step - loss: 0.6400 - accuracy: 0.8289 - val_loss: 0.6634 - val_accuracy: 0.8311\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 485s 621ms/step - loss: 0.6370 - accuracy: 0.8302 - val_loss: 0.6337 - val_accuracy: 0.8400\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 481s 616ms/step - loss: 0.6379 - accuracy: 0.8298 - val_loss: 0.6717 - val_accuracy: 0.8300\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 484s 620ms/step - loss: 0.6325 - accuracy: 0.8303 - val_loss: 0.5967 - val_accuracy: 0.8496\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 484s 619ms/step - loss: 0.6333 - accuracy: 0.8328 - val_loss: 0.5968 - val_accuracy: 0.8476\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 482s 617ms/step - loss: 0.6240 - accuracy: 0.8346 - val_loss: 0.6157 - val_accuracy: 0.8456\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 483s 619ms/step - loss: 0.6270 - accuracy: 0.8340 - val_loss: 0.6219 - val_accuracy: 0.8440\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 482s 617ms/step - loss: 0.6267 - accuracy: 0.8351 - val_loss: 0.5904 - val_accuracy: 0.8565\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 485s 622ms/step - loss: 0.6234 - accuracy: 0.8354 - val_loss: 0.6108 - val_accuracy: 0.8511\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 485s 621ms/step - loss: 0.6154 - accuracy: 0.8406 - val_loss: 0.6338 - val_accuracy: 0.8407\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 483s 619ms/step - loss: 0.6143 - accuracy: 0.8383 - val_loss: 0.6075 - val_accuracy: 0.8464\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 484s 620ms/step - loss: 0.6123 - accuracy: 0.8398 - val_loss: 0.6332 - val_accuracy: 0.8398\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 481s 616ms/step - loss: 0.6149 - accuracy: 0.8401 - val_loss: 0.6345 - val_accuracy: 0.8404\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 482s 617ms/step - loss: 0.6140 - accuracy: 0.8392 - val_loss: 0.7043 - val_accuracy: 0.8289\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 486s 623ms/step - loss: 0.6070 - accuracy: 0.8424 - val_loss: 0.6554 - val_accuracy: 0.8351\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 483s 619ms/step - loss: 0.6027 - accuracy: 0.8424 - val_loss: 0.5442 - val_accuracy: 0.8671\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 484s 620ms/step - loss: 0.6060 - accuracy: 0.8423 - val_loss: 0.6549 - val_accuracy: 0.8384\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 483s 619ms/step - loss: 0.6076 - accuracy: 0.8422 - val_loss: 0.6276 - val_accuracy: 0.8393\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 488s 625ms/step - loss: 0.6017 - accuracy: 0.8449 - val_loss: 0.5934 - val_accuracy: 0.8548\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 488s 624ms/step - loss: 0.5995 - accuracy: 0.8438 - val_loss: 0.6347 - val_accuracy: 0.8397\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 482s 617ms/step - loss: 0.6044 - accuracy: 0.8424 - val_loss: 0.6249 - val_accuracy: 0.8452\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 483s 618ms/step - loss: 0.6006 - accuracy: 0.8449 - val_loss: 0.5986 - val_accuracy: 0.8505\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 480s 614ms/step - loss: 0.6048 - accuracy: 0.8425 - val_loss: 0.5629 - val_accuracy: 0.8623\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 483s 618ms/step - loss: 0.6027 - accuracy: 0.8451 - val_loss: 0.5757 - val_accuracy: 0.8583\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 489s 626ms/step - loss: 0.5941 - accuracy: 0.8479 - val_loss: 0.5832 - val_accuracy: 0.8556\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 485s 620ms/step - loss: 0.5968 - accuracy: 0.8474 - val_loss: 0.5714 - val_accuracy: 0.8580\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 487s 623ms/step - loss: 0.5932 - accuracy: 0.8485 - val_loss: 0.5425 - val_accuracy: 0.8697\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 491s 629ms/step - loss: 0.5988 - accuracy: 0.8468 - val_loss: 0.5813 - val_accuracy: 0.8550\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 487s 624ms/step - loss: 0.5903 - accuracy: 0.8492 - val_loss: 0.5813 - val_accuracy: 0.8548\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 488s 625ms/step - loss: 0.5957 - accuracy: 0.8477 - val_loss: 0.6055 - val_accuracy: 0.8530\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 489s 626ms/step - loss: 0.5880 - accuracy: 0.8511 - val_loss: 0.5222 - val_accuracy: 0.8710\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 493s 632ms/step - loss: 0.5894 - accuracy: 0.8487 - val_loss: 0.6014 - val_accuracy: 0.8579\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 489s 626ms/step - loss: 0.5923 - accuracy: 0.8499 - val_loss: 0.5842 - val_accuracy: 0.8583\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 491s 629ms/step - loss: 0.5935 - accuracy: 0.8487 - val_loss: 0.6524 - val_accuracy: 0.8367\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 488s 624ms/step - loss: 0.5863 - accuracy: 0.8516 - val_loss: 0.5784 - val_accuracy: 0.8585\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 490s 627ms/step - loss: 0.5875 - accuracy: 0.8508 - val_loss: 0.5807 - val_accuracy: 0.8614\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 493s 631ms/step - loss: 0.5896 - accuracy: 0.8519 - val_loss: 0.5755 - val_accuracy: 0.8613\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 489s 627ms/step - loss: 0.5808 - accuracy: 0.8521 - val_loss: 0.6203 - val_accuracy: 0.8519\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 498s 638ms/step - loss: 0.5882 - accuracy: 0.8510 - val_loss: 0.5463 - val_accuracy: 0.8686\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 498s 638ms/step - loss: 0.5852 - accuracy: 0.8522 - val_loss: 0.6588 - val_accuracy: 0.8408\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 498s 637ms/step - loss: 0.5790 - accuracy: 0.8549 - val_loss: 0.6108 - val_accuracy: 0.8589\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 495s 634ms/step - loss: 0.5816 - accuracy: 0.8529 - val_loss: 0.5927 - val_accuracy: 0.8571\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 489s 626ms/step - loss: 0.5737 - accuracy: 0.8571 - val_loss: 0.5922 - val_accuracy: 0.8579\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 491s 629ms/step - loss: 0.5802 - accuracy: 0.8538 - val_loss: 0.6294 - val_accuracy: 0.8475\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 492s 630ms/step - loss: 0.5830 - accuracy: 0.8520 - val_loss: 0.5973 - val_accuracy: 0.8514\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 493s 631ms/step - loss: 0.5793 - accuracy: 0.8528 - val_loss: 0.5739 - val_accuracy: 0.8621\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 494s 632ms/step - loss: 0.5798 - accuracy: 0.8528 - val_loss: 0.5811 - val_accuracy: 0.8570\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 492s 629ms/step - loss: 0.5785 - accuracy: 0.8538 - val_loss: 0.5921 - val_accuracy: 0.8558\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 495s 634ms/step - loss: 0.5776 - accuracy: 0.8545 - val_loss: 0.5642 - val_accuracy: 0.8655\n",
      "Epoch 75/125\n",
      "526/781 [===================>..........] - ETA: 2:31 - loss: 0.5723 - accuracy: 0.8536Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    " \n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.summary()\n",
    " \n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    " \n",
    "#training\n",
    "batch_size = 64\n",
    " \n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "#save to disk\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5') \n",
    " \n",
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "colab_type": "code",
    "id": "VF2m6w1ZoU6H",
    "outputId": "969ad76c-56b5-4712-929c-8ae7e645f78e"
   },
   "outputs": [
    {
     "ename": "DisabledFunctionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1e456f0bb5f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sampleimage.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#show_img()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    " \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "# mean-std normalization\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "show_img(x_test[:16])\n",
    " \n",
    "# Load trained CNN model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('model.h5')\n",
    " \n",
    "labels =  ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    " \n",
    "indices = np.argmax(model.predict(x_test[:16]),1)\n",
    "print ([labels[x] for x in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhVxsZ_VjZp0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CIFAR10-15 Layers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
